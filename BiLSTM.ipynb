{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52167d6a-3b42-4ba3-b8b8-69e6faadbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf3b34c-b0dc-4aba-85d6-334d3b33846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3bfaa4c-cd48-4860-973e-8aef0004ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "fake_df = pd.read_csv(\"archive2/Fake.csv\")\n",
    "true_df = pd.read_csv(\"archive2/True.csv\")\n",
    "\n",
    "fake_df[\"label\"] = 0\n",
    "true_df[\"label\"] = 1\n",
    "\n",
    "df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad91a80-7dfb-4ddb-842d-52b27834d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text preprocessing\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a512098d-412c-4438-a545-5dd40c064a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizd\\anaconda3\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenized_texts = df[\"text\"].apply(tokenize)\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_texts, specials=[\"<pad>\", \"<unk>\"], max_tokens=10000)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a30162-53ad-49dd-9d19-3ccb258c4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequences\n",
    "encoded_texts = [torch.tensor(vocab(tokens)) for tokens in tokenized_texts]\n",
    "max_len = 500\n",
    "padded_texts = pad_sequence(encoded_texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "padded_texts = padded_texts[:, :max_len]  # trim if longer\n",
    "\n",
    "labels = torch.tensor(df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f2a12a-0aa1-4b6c-8bbe-4e54e2ee7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50ec755-181c-4ea6-8d8e-639f6df31777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = NewsDataset(X_train, y_train)\n",
    "test_dataset = NewsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da49a44a-713f-4762-a468-c47601739851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BiLSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        x = self.dropout(hidden)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze()\n",
    "model = BiLSTM(vocab_size=len(vocab), embed_dim=128, hidden_dim=64).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab52ce8d-0a0b-4133-b406-66cacc2ce1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [04:23<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [03:41<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [04:13<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [04:24<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [04:21<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in tqdm(train_loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
